{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install geocoder\n",
    "!pip install lxml html5lib\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geocoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tnrange, tqdm_notebook\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"p\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# what we'll eventually export\n",
    "dfs_2015_2016 = []\n",
    "dfs_all = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cache place queries  latlng \n",
    "geo_cache = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# take a string, get a latlng, with this simple cache\n",
    "def get_lat_lng(x):\n",
    "    debug = False\n",
    "#   tqdm.write(\"Done task %i\" % i)\n",
    "    if debug: print('\\n'+x)\n",
    "    if x in geo_cache:\n",
    "        res = geo_cache[x]\n",
    "    g = geocoder.google(x)\n",
    "    if g.latlng:\n",
    "        geo_cache[x] = g.latlng\n",
    "        res = g.latlng\n",
    "    else:\n",
    "        res = [pd.np.nan, pd.np.nan]\n",
    "    if debug: print(res)\n",
    "    return res\n",
    "\n",
    "# examples\n",
    "# print(get_lat_lng('Brownstone Bagel, Brooklyn, NY'))\n",
    "# print(get_lat_lng('5th Avenue and Union Ave, Brooklyn, NY'))\n",
    "# print(get_lat_lng('Tower Building Services Inc., New York, NY 10021'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crime Deaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO need to do paging, this just squeaks under the 50k limit XXX \n",
    "crime_deaths_url = \"https://data.cityofnewyork.us/resource/n98d-maqp.json?ofns_desc=MURDER%20%26%20NON-NEGL.%20MANSLAUGHTER&$limit=50000\"\n",
    "r = requests.get(crime_deaths_url)\n",
    "crime_deaths_json = r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(crime_deaths_json))\n",
    "# print(crime_deaths_json[0])\n",
    "crime_deaths = pd.DataFrame.from_records(crime_deaths_json)\n",
    "# crime_deaths\n",
    "crime_deaths[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "crime_deaths.drop(['ofns_desc'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "crime_deaths[\"kind\"] = \"c\"\n",
    "crime_deaths\n",
    "crime_deaths.columns = ['lat', 'lng', 'date', 'kind']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(crime_deaths.isnull().sum())\n",
    "# crime_deaths\n",
    "# returns 2 crime deaths we don't have lat lng for, but they lack any location info, nothing to geocode\n",
    "# there's also no descriptive information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "crime_deaths['date']= pd.to_datetime(crime_deaths['date'])\n",
    "crime_deaths = crime_deaths.set_index((crime_deaths.select_dtypes(include=[np.datetime64]).columns).tolist())\n",
    "# crime_deaths\n",
    "# crime_deaths['date']['2015']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "crime_deaths.sort_index(inplace=True)\n",
    "\n",
    "# add to our stuff we want to output\n",
    "dfs_2015_2016.append( crime_deaths['2015':'2016'])\n",
    "dfs_all.append(crime_deaths)\n",
    "\n",
    "# .to_csv(\"cd.csv\")\n",
    "# deaths.to_json(\"deaths.json\", orient=\"records\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dfs_2015_2016[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motor vehicle related deaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "vehicle_deaths_url = \"https://data.cityofnewyork.us/resource/arr5-wtax.json?&$limit=50000\"\n",
    "r = requests.get(vehicle_deaths_url)\n",
    "vehicle_deaths_json = r.json()\n",
    "vehicle_deaths = pd.DataFrame.from_records(vehicle_deaths_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# how many have missing data\n",
    "# vehicle_deaths.isnull().sum()\n",
    "# show those missing lat\n",
    "# vehicle_deaths[vehicle_deaths.isnull()['latitude']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# vehicle_deaths[:1]\n",
    "vehicle_deaths.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# specific to motor vehicle data / rows\n",
    "def maybe_get_location(row):\n",
    "    borough = row.borough if row.borough is not pd.np.NaN else ''\n",
    "    if row.latitude is not pd.np.NaN :\n",
    "        return [row.latitude, row.longitude]\n",
    "    elif row.cross_street_name is not pd.np.NaN:\n",
    "        return get_lat_lng(row.cross_street_name+' '+borough+ ' New York')\n",
    "    elif row.off_street_name is not pd.np.NaN:\n",
    "        return get_lat_lng(row.off_street_name+' '+borough+ ' New York')\n",
    "    elif row.on_street_name is not pd.np.NaN:\n",
    "        return get_lat_lng(row.on_street_name+' '+borough+ ' New York')\n",
    "    else:\n",
    "        # print('no dice for ', row)\n",
    "        return [pd.np.nan, pd.np.nan]\n",
    "\n",
    "    \n",
    "# https://stackoverflow.com/a/26887820/83859\n",
    "# df.apply (lambda row: label_race (row),axis=1) # just to see result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pass it the row\n",
    "mvd_locs = vehicle_deaths.apply (lambda row: maybe_get_location (row),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# turn the series of lists [lat,lng] into it's own frame, i.e. two series\n",
    "mvd_loc_df = mvd_locs.progress_apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# vehicle_deaths[vehicle_deaths.isnull()['latitude']]\n",
    "mvd_loc_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add locations from our other table\n",
    "vehicle_deaths['latitude'] = mvd_loc_df[0]\n",
    "vehicle_deaths['longitude'] = mvd_loc_df[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vehicle_deaths.info()\n",
    "vehicle_deaths['date']= pd.to_datetime(vehicle_deaths['date'])\n",
    "vehicle_deaths = vehicle_deaths.set_index((vehicle_deaths.select_dtypes(include=[np.datetime64]).columns).tolist())\n",
    "\n",
    "# just added weds XXX\n",
    "vehicle_deaths.sort_index(inplace=True)\n",
    "\n",
    "# vehicle_deaths = crime_deaths.set_index((vehicle_deaths.select_dtypes(include=[np.datetime64]).columns).tolist())\n",
    "vehicle_deaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vehicle_deaths.contributing_factor_vehicle_1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_vehicle_death_description(row):\n",
    "    res = ''\n",
    "#     print(row.number_of_cyclist_killed)\n",
    "#     borough = row.borough if row.borough is not pd.np.NaN else ''\n",
    "    \n",
    "#     factors = row.contributing_factor_vehicle_1+', '+row.contributing_factor_vehicle_2 if row.contributing_factor_vehicle_2 is not pd.np.NaN else row.contributing_factor_vehicle_1\n",
    "    if row.number_of_cyclist_killed is not '0':\n",
    "        res+= row.number_of_cyclist_killed \n",
    "        res+=' cyclist, ' if row.number_of_cyclist_killed is '1' else ' cyclists, '\n",
    "    if row.number_of_motorist_killed is not '0':\n",
    "        res+= row.number_of_motorist_killed \n",
    "        res+= ' motorist, ' if row.number_of_motorist_killed is '1' else ' motorists, '\n",
    "    if row.number_of_pedestrians_killed is not '0':\n",
    "        res+= row.number_of_pedestrians_killed\n",
    "        res+= ' pedestrian, ' if row.number_of_pedestrians_killed == '1' else ' pedestrians ,'\n",
    "\n",
    "#     factors = ''\n",
    "#     print(row.contributing_factor_vehicle_1 != 'Unspecified' and row.contributing_factor_vehicle_1 is not pd.np.NaN)\n",
    "    if (row.contributing_factor_vehicle_1 != 'Unspecified') and row.contributing_factor_vehicle_1 is not pd.np.NaN :\n",
    "        factors= row.contributing_factor_vehicle_1+', '+row.contributing_factor_vehicle_2 if row.contributing_factor_vehicle_2 is not pd.np.NaN and row.contributing_factor_vehicle_2 != 'Unspecified' else row.contributing_factor_vehicle_1\n",
    "    else :\n",
    "        factors = ''\n",
    "    final = res+factors\n",
    "    final = final.strip()\n",
    "    if final.endswith(','):\n",
    "        final = final[:-1]\n",
    "    return final.strip()\n",
    "\n",
    "# vehicle_deaths.apply (lambda row: make_vehicle_death_description(row),axis=1)\n",
    "vehicle_deaths['description'] = vehicle_deaths.apply (lambda row: make_vehicle_death_description(row),axis=1)\n",
    "vehicle_deaths\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# included 'location', but dropped online, it's just lat lng as a string, and blank if they're blank\n",
    "# todo drop rest of these, create descriptions\n",
    "vehicle_deaths.drop(['borough',  'cross_street_name', 'off_street_name', 'on_street_name', 'zip_code', 'contributing_factor_vehicle_1', 'contributing_factor_vehicle_2', 'number_of_cyclist_killed', 'number_of_motorist_killed', 'number_of_pedestrians_killed' ], axis = 1, inplace = True)\n",
    "# 'date' doesn't need to be there I guess? \n",
    "vehicle_deaths.columns = ['lat','lng', 'description' ] \n",
    "vehicle_deaths['kind']= 'v'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# vehicle_deaths['2015':'2016']\n",
    "dfs_2015_2016.append( vehicle_deaths['2015':'2016'])\n",
    "dfs_all.append(vehicle_deaths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deaths by Police\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO \n",
    "# make date based\n",
    "# add description, just use classification, age gender\n",
    "\n",
    "all_by_police = pd.concat([pd.read_csv('https://raw.githubusercontent.com/flother/thecounted/master/data/the-counted-2016.csv'),  pd.read_csv('https://raw.githubusercontent.com/flother/thecounted/master/data/the-counted-2015.csv')] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ny_by_police = all_by_police[all_by_police['state']=='NY']\n",
    "depts = ['New York Police Department', 'New York City Police Department', 'New York State Police', 'New York court officer']\n",
    "ny_by_police = ny_by_police[ny_by_police['lawenforcementagency'].isin(depts)]\n",
    "not_nyc = ['Berne', 'Catskill', 'Mount Vernon']\n",
    "ny_by_police = ny_by_police[~ny_by_police['city'].isin(not_nyc)]\n",
    "ny_by_police[:1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_location_police(row):\n",
    "    return get_lat_lng(row.streetaddress+' '+row.city+ ' '+ row.state)\n",
    "# pass it the row\n",
    "by_police_loc = ny_by_police.progress_apply (lambda row: get_location_police (row),axis=1).apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# by_police_loc\n",
    "# turn the series of lists [lat,lng] into it's own frame, i.e. two series\n",
    "# mvd_loc_df = mvd_locs.apply(pd.Series)\n",
    "ny_by_police['lat'] = by_police_loc[0]\n",
    "ny_by_police['lng'] = by_police_loc[1]\n",
    "# ny_by_police[:1]\n",
    "ny_by_police.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ny_by_police[:1]\n",
    "ny_by_police['date'] = ny_by_police.apply (lambda row:  row.month+' '+str(row.day)+' '+ str(row.year), axis=1)\n",
    "ny_by_police['date']= pd.to_datetime(ny_by_police['date'])\n",
    "ny_by_police = ny_by_police.set_index((ny_by_police.select_dtypes(include=[np.datetime64]).columns).tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ny_by_police.age[0]\n",
    "def make_by_police_description(row):\n",
    "    eth = row.raceethnicity.lower()+' ' if row.raceethnicity.lower() is not 'unknown' else ''\n",
    "    return row.age+ ' year old '+eth+row.gender.lower()+ ' - '+ row.classification+''\n",
    "\n",
    "ny_by_police['description'] = ny_by_police.apply (lambda row: make_by_police_description(row),axis=1)\n",
    "ny_by_police"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ny_by_police = ny_by_police[['lat', 'lng', 'date']]\n",
    "#  just use drop, that creates a warning, \n",
    "ny_by_police['kind'] = 'b'\n",
    "p_cols_to_drop = ['uid', 'name', 'age', 'gender', 'raceethnicity', 'month', 'day', 'year', 'streetaddress', 'city', 'state', 'classification','lawenforcementagency', 'armed']\n",
    "ny_by_police.drop(p_cols_to_drop, axis = 1, inplace = True)\n",
    "ny_by_police[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ny_by_police.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# only 2015 2016 anyway, so it doesn't matter but\n",
    "dfs_2015_2016.append(ny_by_police)\n",
    "dfs_all.append(ny_by_police)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ny_by_police[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OSHA Part 1: XML of recent fatalities and catastrophe\n",
    "The recent stuff is in xml, the older stuff is csv, don't worry, they don't have the same fields either"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/a/10077069/83859\n",
    "from xml.etree import ElementTree\n",
    "from collections import defaultdict\n",
    "\n",
    "def etree_to_dict(t):\n",
    "    d = {t.tag: {} if t.attrib else None}\n",
    "    children = list(t)\n",
    "    if children:\n",
    "        dd = defaultdict(list)\n",
    "        for dc in map(etree_to_dict, children):\n",
    "            for k, v in dc.items():\n",
    "                dd[k].append(v)\n",
    "        d = {t.tag: {k:v[0] if len(v) == 1 else v for k, v in dd.items()}}\n",
    "    if t.attrib:\n",
    "        d[t.tag].update(('@' + k, v) for k, v in t.attrib.items())\n",
    "    if t.text:\n",
    "        text = t.text.strip()\n",
    "        if children or t.attrib:\n",
    "            if text:\n",
    "              d[t.tag]['#text'] = text\n",
    "        else:\n",
    "            d[t.tag] = text\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "response = requests.get('https://www.osha.gov/dep/fatcat/fatcats.xml')\n",
    "# print(response)\n",
    "\n",
    "tree = ElementTree.fromstring(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', 800)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this has 2016 and 2017, in xml\n",
    "osha_current = pd.DataFrame.from_records( etree_to_dict(tree)['events']['incident'] )\n",
    "# print(osha_current[:4].employer[3])\n",
    "# osha_current[:1]\n",
    "\n",
    "# just ny state for (mostly)\n",
    "osha_current = osha_current[osha_current.employer.str.contains('NY')]\n",
    "# drop what we're not using\n",
    "osha_current.drop(['fatcat', 'inspection', 'victim'], axis=1, inplace=True)\n",
    "osha_current[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_osha_loc(row):    \n",
    "    return get_lat_lng(row.employer.split(',')[-1].strip())\n",
    "#     \"MHP Real Estate Services, 180 Maiden Lane NEW YO\".split(',')[-1].strip()\n",
    "# osha_current[lat]\n",
    "osha_current_loc = osha_current.progress_apply (lambda row: get_osha_loc(row),axis=1).apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# osha_current_loc[osha_current_loc.duplicated(keep=False)]\n",
    "osha_current['lat'] = osha_current_loc[0]\n",
    "osha_current['lng'] = osha_current_loc[1]\n",
    "osha_current[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "osha_current['description'] = osha_current.apply(lambda row: row.description+' - '+row.employer.split(',')[0], axis=1) \n",
    "\n",
    "# ny_by_police['date'] = ny_by_police.apply (lambda row:  row.month+' '+str(row.day)+' '+ str(row.year), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "osha_current.drop(['employer'], axis=1, inplace=True)\n",
    "osha_current['kind'] = 'w'\n",
    "osha_current[:3]\n",
    "# osha_current[:1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "osha_current['date']= pd.to_datetime(osha_current['date'])\n",
    "osha_current = osha_current.set_index((osha_current.select_dtypes(include=[np.datetime64]).columns).tolist())\n",
    "osha_current.sort_index(inplace=True)\n",
    "osha_current[:3]\n",
    "# ok next combine that with the other osha stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "osha_current[-3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### OSHA Part 2, Archive CSVs 2015 and Before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "osha_archive_urls = [\n",
    "\"https://www.osha.gov/dep/fatcat/FatalitiesFY09.csv\",\n",
    "\"https://www.osha.gov/dep/fatcat/FatalitiesFY10.csv\",\n",
    "\"https://www.osha.gov/dep/fatcat/FatalitiesFY11.csv\",\n",
    "\"https://www.osha.gov/dep/fatcat/FatalitiesFY12.csv\",\n",
    "\"https://www.osha.gov/dep/fatcat/fy13_federal-state_summaries.csv\",\n",
    "\"https://www.osha.gov/dep/fatcat/fy14_federal-state_summaries.csv\",\n",
    "\"https://www.osha.gov/dep/fatcat/fy15_federal-state_summaries.csv\"]\n",
    "\n",
    "osha_archive = pd.DataFrame()\n",
    "for url in tqdm(osha_archive_urls):\n",
    "    df = pd.DataFrame.from_csv(url)\n",
    "#     print(df.info())\n",
    "    osha_archive = pd.concat([osha_archive, df])\n",
    "#     osha_current = pd.concat([osha_current, df])\n",
    "#     print(url)\n",
    "# all_osha.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "osha_archive.dropna(axis=1, how=\"all\", inplace=True)\n",
    "osha_archive[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# need to fix the fy date\n",
    "osha_archive.reset_index(level=0,  inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fill place with either company or company city state field\n",
    "osha_archive['place'] = osha_archive['Company'].fillna(osha_archive['Company, City, State, ZIP'])\n",
    "# drop ones we don't have place field for\n",
    "osha_archive = osha_archive.dropna(axis=0, subset=['place'])\n",
    "# only include the ones with NY in the string, not the best test\n",
    "osha_archive = osha_archive[osha_archive.place.str.contains('NY')]\n",
    "# osha_archive[2:]\n",
    "# osha_archive[osha_archive.isnull()['Date of Incident'] ]\n",
    "# osha_archive.info()\n",
    "osha_archive['Date of Incident'] = osha_archive['Date of Incident'].fillna( osha_archive['index'] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_osha_archive_loc(row):\n",
    "\n",
    "    return get_lat_lng(row['place'])    \n",
    "\n",
    "osha_archive_loc = osha_archive.apply (lambda row: get_osha_archive_loc(row),axis=1).apply(pd.Series)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# osha_archive_loc.apply(pd.Series)\n",
    "osha_archive['lat']=osha_archive_loc[0]\n",
    "osha_archive['lng']=osha_archive_loc[1]\n",
    "\n",
    "osha_archive['Date of Incident']= pd.to_datetime(osha_archive['Date of Incident'])\n",
    "osha_archive = osha_archive.set_index((osha_archive.select_dtypes(include=[np.datetime64]).columns).tolist())\n",
    "osha_archive.sort_index(inplace=True)\n",
    "# untried, Weds XXX\n",
    "osha_archive.index.rename('date', inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "osha_archive['Preliminary Description of Incident'].fillna('', inplace=True)\n",
    "osha_archive['Company'].fillna('', inplace=True)\n",
    "osha_archive.info()\n",
    "# same for Company, and apply it, so no error below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# todo add descrip\n",
    "# osha_archive\n",
    "osha_archive['description'] = osha_archive.apply(lambda row: row['Preliminary Description of Incident']+' - '+row['Company'].split(',')[0], axis=1) \n",
    "osha_archive[:1]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# osha_archive.columns\n",
    "osha_archive.drop(['index', 'Company', 'Company, City, State, ZIP',\n",
    "       'Fatality or Catastrophe', 'Inspection #',\n",
    "       'Preliminary Description of Incident', 'Summary Report Date',\n",
    "       'Unnamed: 4', 'Victim(s)', 'place'], axis=1, inplace=True)\n",
    "osha_archive[:1]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "osha_archive[:10]\n",
    "# osha_archive.columns = ['date', 'lat', 'lng']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "osha_archive['kind'] = 'w'\n",
    "osha_archive.dropna(how='any', inplace=True)\n",
    "osha_archive.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# combine all the osha records, before splitting by time below\n",
    "osha_all = pd.concat([osha_archive, osha_current])\n",
    "# there are a couple with problems\n",
    "osha_all.dropna(axis=0, how='any', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for the end result\n",
    "dfs_2015_2016.append(osha_all['2015':'2016'])\n",
    "dfs_all.append(osha_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# combine them!\n",
    "# deaths_all_years = pd.concat([vehicle_deaths, crime_deaths, ny_by_police, osha_all], ignore_index=True)\n",
    "recent_deaths = pd.concat(dfs_2015_2016)\n",
    "# recent_deaths.info()\n",
    "# print('z')\n",
    "# don't drop if we lack description\n",
    "\n",
    "recent_deaths.dropna(axis=0, inplace=True, subset=['kind', 'lat', 'lng'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_deaths = pd.concat(dfs_all)\n",
    "# recent_deaths.info()\n",
    "# print('z')\n",
    "# don't drop if we lack description\n",
    "all_deaths.dropna(axis=0, inplace=True, subset=['kind', 'lat', 'lng'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "recent_deaths.sort_index(axis=0, inplace=True)\n",
    "all_deaths.sort_index(axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_deaths.info() #.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recent_deaths.to_csv('recent_deaths.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_deaths.to_csv('all_deaths.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# deaths.columns = ['d', 'k', 'lat', 'lng' ]\n",
    "# deaths.to_json(\"deaths.json\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# deaths.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# deaths.lat.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
